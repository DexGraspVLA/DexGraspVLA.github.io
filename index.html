<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</title>

    <!-- 添加favicon -->
    <link rel="icon" type="image/x-icon" href="assets/images/favicon.jpg">
    <!-- 或者使用PNG格式 -->
    <!-- <link rel="icon" type="image/png" href="assets/favicon.png"> -->

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-2 publication-title">DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</h1>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                          <a target="_blank" href="https://ivan-zhong.github.io/">Yifan Zhong</a><sup>1,2*</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://dexgraspvla.github.io/">Xuchuan Huang</a><sup>1,2*</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://dexgraspvla.github.io/">Ruochong Li</a><sup>2,3</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://scholar.google.com/citations?user=OadTFGMAAAAJ&hl=zh-CN">Ceyao Zhang</a><sup>1,2</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://scholar.google.com/citations?user=KVzR1XEAAAAJ&hl=en">Yitao Liang</a><sup>1,2</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://www.yangyaodong.com/">Yaodong Yang</a><sup>1,2†</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://cypypccpy.github.io/">Yuanpei Chen</a><sup>1,2†</sup>
                        </span>

          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute for AI, Peking University, <sup>2</sup>PKU-PsiBot Joint Lab, <sup>3</sup>HKUST (Guangzhou)</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution, <sup>†</sup>Corresponding authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="assets/paper/DexGraspVLA.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Arxiv Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://arxiv.org/abs/2411.04005"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>ArXiv</span>
              </a>
            </span> -->

            <!-- Video Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://youtu.be/2mmqQYO4KlY"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span> -->
            <!-- Twitter Link. -->
            <!-- <span class="link-block">
              <a href="https://twitter.com/chenwang_j/status/1699453998464667814" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-twitter"></i>
                </span>
                <span>Twitter</span>
              </a>
            </span> -->
            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/Psi-Robot/DexGraspVLA"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>
            </div>
          </div>
        </div>
    </div>
</section>

<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <!-- Paper video -->
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <div class="publication-video" style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; margin-bottom: 20px; width: 900px; margin-left: auto; margin-right: auto;">
            <iframe 
              style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"
              src="https://www.youtube.com/embed/X0Sq7q-bfI8"
              frameborder="0" 
              allow="autoplay; encrypted-media" 
              allowfullscreen>
            </iframe>
          </div>
        </div>
      </div>
      <!-- /Paper video -->
  </div>
</section>

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-build_pymarid">
          <video poster="" id="build_pymarid" autoplay controls muted loop height="100%">
            <source src="assets/videos/grasp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-build_a_tower">
          <video poster="" id="build_a_tower" autoplay controls muted loop height="100%">
            <source src="assets/videos/grasp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-Insert_a_red">
          <video poster="" id="Insert_a_red" autoplay controls muted loop height="100%">
            <source src="assets/videos/grasp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-stack_two_cube">
          <video poster="" id="stack_two_cube" autoplay controls muted loop height="100%">
            <source src="assets/videos/grasp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-disturbance">
          <video poster="" id="disturbance" autoplay controls muted loop height="100%">
            <source src="assets/videos/grasp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-disturbance"></div>
          <video poster="" id="disturbance" autoplay controls muted loop height="100%">
            <source src="assets/videos/grasp.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <h2 class="subtitle has-text-centered">
      </br>
      Despite being trained only in simulation, our system demonstrates <b>zero-shot transfer</b> to two real-world robots equipped with the dexterous hand.
      </h2>
    </div>
  </div>
</section> -->

<section class="hero is-light">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="font-size: 125%">
            Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on specific assumptions, such as single-object settings or limited environments, leading to constrained generalization. Our solution is DexGraspVLA, a hierarchical framework that utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller. The key insight lies in iteratively transforming diverse language and visual inputs into domain-invariant representations, where imitation learning can be effectively applied due to the alleviation of domain shift. Thus, it enables robust generalization across a wide range of real-world scenarios. Notably, our method achieves a 90+% success rate under thousands of unseen object, lighting, and background combinations in a "zero-shot" environment. Empirical analysis further confirms the consistency of internal model behavior across environmental variations, thereby validating our design and explaining its generalization performance. We hope our work can be a step forward in achieving general dexterous grasping.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <img src="assets/images/method.jpg" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/>
               <div class="content has-text-justified">
                <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
                  <p>DexGraspVLA adopts a hierarchical architecture composed of an off-the-shelf VLM-based high-level 
                  <span style="color:#6a7bff; font-weight:bold;">planner</span> and a diffusion-based low-level <span style="color:#ff6b6b; font-weight:bold;">controller</span>. 
                  Given a cluttered scene, the <span style="color:#6a7bff; font-weight:bold;">planner</span> reasons about the user prompt, e.g., <i>"clear the table"</i>, decomposing it into multiple grasping instructions when necessary. 
                  For each instruction \(l\), e.g., <i>"grasp the cookie"</i>, the <span style="color:#6a7bff; font-weight:bold;">planner</span> identifies the target object \(A\) from the head image \(\mathbf{I}_{t_0}\) 
                  and marks its bounding box \((x_1^A, y_1^A, x_2^A, y_2^A)\) at initial time \(t_0\).</p>
        
                  <p>The <span style="color:#ff6b6b; font-weight:bold;">controller</span> consists of four parts:</p>
                  <ol>
                    <li>Two segmentation models including SAM, which obtains the object's mask \(\mathbf{m}_{t_0}\) at \(t_0\), and Cutie, a video segmentation model that continuously tracks the mask \(\mathbf{m}_t\) during each grasping process.</li>
                    <li>Three vision encoders including two frozen DINOv2 that extract features from the third-view head-camera image \(\mathbf{I}_t^h\) and the first-view wrist-camera image \(\mathbf{I}_t^w\), and a trainable ViT that deals with the mask \(\mathbf{m}_t\).</li>
                    <li>Three MLP projectors that map the visual features and robot proprioceptive state into the same feature space, forming a feature sequence.</li>
                    <li>A DiT that predicts an action chunk from \(\mathbf{a}_t\) to \(\mathbf{a}_{t+H-1}\).</li>
                  </ol>
        
                  <p>During the <span style="color:#ff6b6b; font-weight:bold;">controller</span>'s grasping process, the <span style="color:#6a7bff; font-weight:bold;">planner</span> monitors the execution, 
                  checks whether grasping succeeds, and assists re-grasping when failing. This process continues until the user prompt is fully completed.</p>
                </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- Generalization Videos -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
          <h3 class="title is-4"><span
            class="dvima">Environment Setups
          </span></h3>

          <img src="assets/images/workspace.jpg" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/>
               <div class="content has-text-justified">
               <br>
          <span style="font-size: 110%">
            Overview of the environment setups: <br>
            <b>(a).</b> Workspace of the simulation. We employ two Shadow Hands, each individually mounted on separate UR10e robots, arranged in an abreast configuration. <br>
            <b>(b).</b> Object sets in the simulation and the real-world. <br>
            <b>(c).</b> Workspace of the real-world, mirroring the simulation, the robot system uses the same Shadow Hands and UR10e robots as the simulation.
            <!-- <b>(a).</b> Workspace of <b>Building Blocks</b> task in simulation and real-world. This long-horizon task includes four different subtasks: Searching for a block with desired dimension and color from a pile of cluttered blocks, Orienting the block to a favorable position, Grasping the block, and finally Inserting the block to its designated position on the structure. This sequence of actions repeats until the structure is completed according to the given assembly instructions. <br>
            <b>(b).</b> The setup of the <b>Tool Positioning</b> task. Initially, the tool is placed on the table in a random pose, and the dexterous hand needs to grasp the tool and re-orient it to a ready-to-use pose. The comparison results illustrate how the way of grasping directly influences subsequent orientation. -->
          <br>
          <br>
        </div>
      </div>
    </div>
  </div>
          <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
              <div class="column">
                  <h3 class="title is-4"><span
                    class="dvima">Import Human Motion Capture Data to Simulation
                  </span></h3>
                  <br>
                  <video poster="" autoplay controls muted loop width="900">
                    <source src="assets/videos/grasp.mp4"
                            type="video/mp4">
                  </video>
          <br>
          <br>

          <div class="content has-text-justified">
            <span style="font-size: 110%">
              The upper left video is the object goal trajectory input, and the upper right video is the high-level planner output (wrist motion generation).  
          </div>

        </div>
        </div>
      </div>

      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column">
              <h3 class="title is-4"><span
                class="dvima">Optimizing Human Mocap Data via Reinforcement Learning
              </span></h3>
              <br>
              <video poster="" autoplay controls muted loop width="900">
                <source src="assets/videos/grasp.mp4"
                        type="video/mp4">
              </video>
      <br>
      <br>

      <div class="content has-text-justified">
        <span style="font-size: 110%">
          The upper left video is the object goal trajectory input, and the upper right video is the high-level planner output (wrist motion generation).  
      </div>

    </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
          <h3 class="title is-4"><span
            class="dvima">Learning Object-Centric Dexterous Manipulation in Isaac-Gym
          </span></h3>
          <br>
          <video poster="" autoplay controls muted loop width="900">
            <source src="assets/videos/grasp.mp4"
                    type="video/mp4">
          </video>
  <br>
  <br>

  <div class="content has-text-justified">
    <span style="font-size: 110%">
      <b>Upper Left Video:</b> Object goal trajectories from Human Mocap Data (ARCTIC Dataset), which are the input of our policy. <br>
      <b>Upper Right Video:</b> High-level planner output (wrist motion generation). Object motion replay for visualization. <br>
      <b>Lower Video:</b> Low-level policy output (finger + wrist motion). Fully autonomous results, no object motion replay.
    </div>

</div>
</div>
</div>

<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column">
        <!-- <h3 class="title is-4"><span
          class="dvima">
        </span></h3> -->
        <br>
        <video poster="" autoplay controls muted loop width="900">
          <source src="assets/videos/grasp.mp4"
                  type="video/mp4">
        </video>
<br>
<br>

</div>
</div>
</div>

<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column">
        <br>
        <video poster="" autoplay controls muted loop width="900">
          <source src="assets/videos/grasp.mp4"
                  type="video/mp4">
        </video>
<br>
<br>

<!-- <div class="content has-text-justified">
  <span style="font-size: 110%">
    The upper left video is the object goal trajectory input, and the upper right video is the high-level planner output (wrist motion generation).  
</div> -->

</div>
</div>
</div>

<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column">
        <br>
        <video poster="" autoplay controls muted loop width="900">
          <source src="assets/videos/grasp.mp4"
                  type="video/mp4">
        </video>
<br>
<br>

</div>
</div>
</div>

<div class="container is-max-desktop"></div>
  <div class="columns is-centered has-text-centered">
    <div class="column">
        <br>
        <video poster="" autoplay controls muted loop width="900">
          <source src="assets/videos/grasp.mp4"
                  type="video/mp4">
        </video>
<br>
<br>

</div>
</div>
</div>


    </div>

                </div>
              </div>
        
            </div>
          </div>
        </div>
      </div>

    </div>
  </div>
</section>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- Generalization Videos -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Additional Experiments</h2>
        </div>
      </div>
    </div>
  </div>
</section>

<!--Additional Experiments-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
          <!-- <h2 class="title is-3"><span
            class="dvima">Additional Experiments</span></h2>
          <br>
          <h3 class="title is-4"><span
            class="dvima">Qualitative results of the learned transition feasibility functions
          </span></h3> -->

          <img src="assets/images/multi_hand.png" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="700"/>
               <div class="content has-text-justified">
                <br>
           <span style="font-size: 110%">
                <b>Experiments on the different embodiment.</b> We apply our method to four different types of multi-fingered dexterous hands, varying in size and degree of freedom. Our method achieved more than 50% completion rate for all hands, demonstrating that our framework can effectively transfer human data to different robot hand embodiments.
          <br>
          <br>

        </div>

          <h3 class="title is-4"><span
            class="dvima">Quantitative results in the Building Blocks task
          </span></h3>

          <img src="assets/images/qual_results.png" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/>

               <br>
               <br>

          </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- Generalization Videos -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Conclusion</h2>
      <div class="column">
          <div class="content has-text-justified">
          <p style="font-size: 125%">
            In this work, we present a hierarchical policy learning framework that effectively utilizes human hand motion data to train object-centric dexterous robot manipulation. At the core of our method is a high-level trajectory generative model trained with a large-scale human hand motion capture dataset, which synthesizes human-like wrist motions conditioned on the object goal trajectory. Guided by these wrist motions, we further trained an RL-based low-level finger controller to achieve the task goal. Our approach demonstrated superior performance across various household objects and showcased generalization capabilities to novel object geometries and goal trajectories. Moreover, the successful transfer of the learned policies from simulation to a real-world bimanual dexterous robot system underscores the practical applicability of our method in real-world scenarios.
        </p>

        </div>
      </div>

    </div>
  </div>
</section>

<!-- <section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{chenobject,
      title={Object-Centric Dexterous Manipulation from Human Motion Data},
      author={Chen, Yuanpei and Wang, Chen and Yang, Yaodong and Liu, Karen},
      booktitle={8th Annual Conference on Robot Learning}
    }</code></pre>
  </div>
</section> -->


</body>
</html>
